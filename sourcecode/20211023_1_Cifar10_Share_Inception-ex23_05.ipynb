{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "IBQ_eVTu8OnB",
    "outputId": "a0117324-8e60-4944-c431-14ed244b4f6e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "LP5lidXD8ViI",
    "outputId": "2ff8199e-96f9-40cd-dd5b-e85cb49178c2"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rYOoAdFk8ma3",
    "outputId": "4e860915-8468-45db-fbcf-be0d20e5c2df"
   },
   "outputs": [],
   "source": [
    "cd /media/datastorage/Phong/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "mDdxdcwj8ozr",
    "outputId": "68605cb6-d5ad-42dd-ce42-3e58e2b7b5db"
   },
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "313ttIKg8riI",
    "outputId": "32ea2f07-35d7-4504-ce4c-2d7aa40c84bb"
   },
   "outputs": [],
   "source": [
    "# #Images/n02105855-Shetland_sheepdog/n02105855_9415.jpg\n",
    "\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# import numpy as np\n",
    "# from matplotlib.image import imread\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # get image parts\n",
    "# def get_image_parts(image_path):\n",
    "#     \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "#     parts = image_path.split(os.path.sep)\n",
    "#     #print(parts)\n",
    "#     filename = parts[2]\n",
    "#     filename_no_ext = filename.split('.')[0]\n",
    "#     classname = parts[1]\n",
    "#     train_or_test = parts[0]\n",
    "    \n",
    "#     return train_or_test, classname, filename_no_ext, filename\n",
    "    \n",
    "    \n",
    "# sample_images = list(glob.glob(os.path.join('train/', '*/*'), recursive=True))\n",
    "# np.random.seed(42)\n",
    "# rand_imgs = np.random.choice(sample_images, size=5*5)\n",
    "# fig, axarr = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "# for i, rand_img in enumerate(rand_imgs):\n",
    "#     train_or_test, classname, filename_no_ext, filename = get_image_parts(rand_img)\n",
    "    \n",
    "#     j = i // 5\n",
    "#     k = i % 5\n",
    "#     axarr[j][k].imshow(imread(rand_img))\n",
    "#     axarr[j][k].title.set_text(classname)\n",
    "#     axarr[j][k].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "UOyt3E-j8tJo",
    "outputId": "969c4599-bc6b-46f0-f648-d2cfca0450b0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Stop training on val_acc\n",
    "class EarlyStoppingByAccVal(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "#Save large model using pickle formate instead of h5            \n",
    "class SaveCheckPoint(Callback):\n",
    "    def __init__(self, model, dest_folder):\n",
    "        super(Callback, self).__init__()\n",
    "        self.model = model\n",
    "        self.dest_folder = dest_folder\n",
    "        \n",
    "        #initiate\n",
    "        self.best_val_acc = 0\n",
    "        self.best_val_loss = sys.maxsize #get max value\n",
    "          \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_acc = logs['val_acc']\n",
    "        val_loss = logs['val_loss']\n",
    "\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            \n",
    "            # Save weights in pickle format instead of h5\n",
    "            print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "            weigh= self.model.get_weights()\n",
    "\n",
    "            #now, use pickle to save your model weights, instead of .h5\n",
    "            #for heavy model architectures, .h5 file is unsupported.\n",
    "            fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "            pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "            fpkl.close()\n",
    "            \n",
    "#             model.save('tmp.h5')\n",
    "        elif val_acc == self.best_val_acc:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss=val_loss\n",
    "                \n",
    "                # Save weights in pickle format instead of h5\n",
    "                print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "                weigh= self.model.get_weights()\n",
    "\n",
    "                #now, use pickle to save your model weights, instead of .h5\n",
    "                #for heavy model architectures, .h5 file is unsupported.\n",
    "                fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "                pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "                fpkl.close()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUL 1 - Inception - ST\n",
    "\n",
    "from keras.applications import InceptionV3\n",
    "# from keras.applications import Xception\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, SimpleRNN, LSTM, Flatten, GRU, Reshape\n",
    "\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "# from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "def get_adv_model():\n",
    "    f1_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299,299,3))  \n",
    "    \n",
    "#     #frozen layers    \n",
    "#     for layer in f1_base.layers:\n",
    "#         layer.trainable = False  \n",
    "        \n",
    "    f1_x = f1_base.output\n",
    "    f1_x = GlobalAveragePooling2D()(f1_x)    \n",
    "    \n",
    "#     f1_base = EfficientNetB0(include_top=False, weights='imagenet', \n",
    "#                     input_shape=(299, 299, 3), \n",
    "#                     pooling='avg')\n",
    "#     f1_x = f1_base.output\n",
    "\n",
    "# f1_x = f1_base.layers[-151].output   #layer 5\n",
    "\n",
    "# f1_x = GlobalAveragePooling2D()(f1_x)\n",
    "# f1_x = Flatten()(f1_x)\n",
    "\n",
    "# f1_x = Reshape([1,1280])(f1_x)  \n",
    "# f1_x = SimpleRNN(2048, \n",
    "#             return_sequences=False,                       \n",
    "# #             dropout=0.8                                     \n",
    "#             input_shape=[1,1280])(f1_x)\n",
    "   \n",
    "    #Regularization with noise\n",
    "    f1_x = GaussianNoise(0.1)(f1_x)\n",
    "\n",
    "    f1_x = Dense(1024, activation='relu')(f1_x)\n",
    "    f1_x = Dense(10, activation='softmax')(f1_x)\n",
    "    model_1 = Model(inputs=[f1_base.input],outputs=[f1_x])\n",
    "    model_1.summary()\n",
    "    \n",
    "    return model_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1E7n8ds9Mmh"
   },
   "outputs": [],
   "source": [
    "# model_mul_test = get_adv_model()\n",
    "# for layer in model_mul_test.layers:\n",
    "#     print(layer.name, ': ', layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FXZBAIXAElcd",
    "outputId": "0d8aaee3-0db2-4b08-c426-d26e581d2c10"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "19V0B0rIEgso",
    "outputId": "78573b3b-934c-45ae-9c10-ccb28e635e0e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "1JMB1lsREbAE",
    "outputId": "1cad26f1-9c6f-4801-bb78-84f43a11e0c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "2suPqhuJDOLn",
    "outputId": "d0b80840-ba14-4470-cb33-c850af69491d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "6SqCtLbgSoQ-",
    "outputId": "2a7c1ac8-a9b2-44c2-c368-91125b14a983"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUUPvXA2S7ev"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UsII2atxTYdG",
    "outputId": "a1c0b166-9ed8-4005-faa8-d2932fa8de5a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BL0e6CLIFwxN",
    "outputId": "960af38c-994c-41b2-9f00-a95d1d56ce6c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA1wZ0ODjKlX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7tCxvDxjNU9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgOQh92Ar10r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Dj-itflssqh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwbSVOPnorCq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "cyl_Xjunolpe",
    "outputId": "0ca332e8-f4f4-4903-c7f3-68c5e41ff860"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "zPyjRcTfytFG",
    "outputId": "2de3a5b1-a9f6-4103-fb2b-631c9e505281"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emlc7dUpq9C1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_sKj8LQqr-g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgL7cGRAqy7p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtFSHddZq16W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGOfqcePBqpH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SjTDA8VC5Ah"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILRxLOABDF7u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "8pDocSJKCtV7",
    "outputId": "a56f8b2d-8684-4f05-e17f-16a84fdd6b3b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WJiLtHmPZ2Ts",
    "outputId": "2e15b430-2dfa-42b8-bc91-6418e714bf62"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "cdFkG0pEacF1",
    "outputId": "6682d252-6c53-4324-a2bf-83d285c6ff27"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41MCKnMGanT1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVSEeQWrbWvn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RM7cE3Skbpw9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# class CutMixImageDataGenerator():\n",
    "#     def __init__(self, generator1, generator2, img_size, batch_size):\n",
    "#         self.batch_index = 0\n",
    "#         self.samples = generator1.samples\n",
    "#         self.class_indices = generator1.class_indices\n",
    "#         self.generator1 = generator1\n",
    "#         self.generator2 = generator2\n",
    "#         self.img_size = img_size\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def reset_index(self):  # Ordering Reset (If Shuffle is True, Shuffle Again)\n",
    "#         self.generator1._set_index_array()\n",
    "#         self.generator2._set_index_array()\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.batch_index = 0\n",
    "#         self.generator1.reset()\n",
    "#         self.generator2.reset()\n",
    "#         self.reset_index()\n",
    "\n",
    "#     def get_steps_per_epoch(self):\n",
    "#         quotient, remainder = divmod(self.samples, self.batch_size)\n",
    "#         return (quotient + 1) if remainder else quotient\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         self.get_steps_per_epoch()\n",
    "\n",
    "#     def __next__(self):\n",
    "#         if self.batch_index == 0: self.reset()\n",
    "\n",
    "#         crt_idx = self.batch_index * self.batch_size\n",
    "#         if self.samples > crt_idx + self.batch_size:\n",
    "#             self.batch_index += 1\n",
    "#         else:  # If current index over number of samples\n",
    "#             self.batch_index = 0\n",
    "\n",
    "#         reshape_size = self.batch_size\n",
    "#         last_step_start_idx = (self.get_steps_per_epoch()-1) * self.batch_size\n",
    "#         if crt_idx == last_step_start_idx:\n",
    "#             reshape_size = self.samples - last_step_start_idx\n",
    "            \n",
    "#         X_1, y_1 = self.generator1.next()\n",
    "#         X_2, y_2 = self.generator2.next()\n",
    "        \n",
    "#         cut_ratio = np.random.beta(a=1, b=1, size=reshape_size)\n",
    "#         cut_ratio = np.clip(cut_ratio, 0.2, 0.8)\n",
    "#         label_ratio = cut_ratio.reshape(reshape_size, 1)\n",
    "#         cut_img = X_2\n",
    "\n",
    "#         X = X_1\n",
    "#         for i in range(reshape_size):\n",
    "#             cut_size = int((self.img_size-1) * cut_ratio[i])\n",
    "#             y1 = random.randint(0, (self.img_size-1) - cut_size)\n",
    "#             x1 = random.randint(0, (self.img_size-1) - cut_size)\n",
    "#             y2 = y1 + cut_size\n",
    "#             x2 = x1 + cut_size\n",
    "#             cut_arr = cut_img[i][y1:y2, x1:x2]\n",
    "#             cutmix_img = X_1[i]\n",
    "#             cutmix_img[y1:y2, x1:x2] = cut_arr\n",
    "#             X[i] = cutmix_img\n",
    "            \n",
    "#         # X = seq.augment_images(X)  # Sequential of imgaug\n",
    "#         y = y_1 * (1 - (label_ratio ** 2)) + y_2 * (label_ratio ** 2)\n",
    "#         return X, y\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         while True:\n",
    "#             yield next(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "P1hxliT6aR_a",
    "outputId": "a359bc33-6afa-4da1-a5fe-13aab3996ef7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import random\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    # preprocessing_function=get_cutout_v2(),\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "NUM_GPU = 1\n",
    "batch_size = 32\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('cifar10/cifar10v3/train_resized_299/',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "valid_set = test_datagen.flow_from_directory('cifar10/cifar10v3/test_resized_299/',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "model_txt = 'st'\n",
    "# Helper: Save the model.\n",
    "savedfilename = os.path.join('cifar10', 'cifar10v3', 'checkpoints', 'Cifar10_Share_1_Inception_ex23_05.hdf5')\n",
    "savedfilename_best = os.path.join('cifar10', 'cifar10v3', 'checkpoints', 'Cifar10_Share_1_Inception_ex23_05_best.hdf5')\n",
    "savedfilename_pre = os.path.join('cifar10', 'cifar10v3', 'checkpoints', 'Cifar10_Share_1_Inception_ex23_05_pre.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_accuracy', verbose=1, \n",
    "                          save_best_only=False, mode='max',save_weights_only=True)########\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('svhn_output', 'logs', model_txt))\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('svhn_output', 'logs', model_txt + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_accuracy', value=0.9900, verbose=1)\n",
    "\n",
    "def rand_scheduler(epoch, lr):\n",
    "    rnd_lr = 10**(random.uniform(np.log10((1e-5)),np.log10((1e-1))))\n",
    "#     if epoch < 30:\n",
    "#         rnd_lr = 1e-2\n",
    "#     else:    \n",
    "#         rnd_lr = 1e-3\n",
    "##     rnd_lr = lr\n",
    "    print('random lr = ', rnd_lr)\n",
    "    return rnd_lr\n",
    "\n",
    "epochs = 40##!!!\n",
    "lr = 1e-2\n",
    "# decay = lr/epochs\n",
    "# optimizer = Adam(lr=lr, decay=decay)\n",
    "# optimizer = Adam(lr=lr)\n",
    "optimizer = SGD(lr=lr)\n",
    "\n",
    "# train on multiple-gpus\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of GPUs: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model_mul = get_adv_model()\n",
    "    model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    # save initial models\n",
    "    model_mul.save_weights(savedfilename)\n",
    "    model_mul.save_weights(savedfilename_best)\n",
    "    model_mul.save_weights(savedfilename_pre)\n",
    "\n",
    "step_size_train=ceil(train_set.n/train_set.batch_size)\n",
    "step_size_valid=ceil(valid_set.n/valid_set.batch_size)\n",
    "# step_size_test=ceil(testing_set.n//testing_set.batch_size)\n",
    "\n",
    "# result = model_mul.fit_generator(\n",
    "#     generator = train_set, \n",
    "#     steps_per_epoch = step_size_train,\n",
    "#     validation_data = valid_set,\n",
    "#     validation_steps = step_size_valid,\n",
    "#     shuffle=True,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=[checkpointer],\n",
    "# #     callbacks=[csv_logger, checkpointer, earlystopping],\n",
    "# #     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "#     verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "d = {'id': [1, 2, 3, 4], 'pbest_value': [0, 0, 0, 0], 'pbest_file':['Cifar10_Share_1_Inception_ex23_05_best.hdf5',\n",
    "                                                        'Cifar10_Share_2_Inception_ex23_05_best.hdf5',\n",
    "                                                        'Cifar10_Share_3_Inception_ex23_05_best.hdf5',\n",
    "                                                        'Cifar10_Share_4_Inception_ex23_05_best.hdf5'], \n",
    "                         'c_value': [0, 0, 0, 0], 'c_file':['Cifar10_Share_1_Inception_ex23_05.hdf5',\n",
    "                                                             'Cifar10_Share_2_Inception_ex23_05.hdf5',\n",
    "                                                             'Cifar10_Share_3_Inception_ex23_05.hdf5',\n",
    "                                                             'Cifar10_Share_4_Inception_ex23_05.hdf5'], \n",
    "                         'pre_value': [0, 0, 0, 0], 'pre_file':['Cifar10_Share_1_Inception_ex23_05_pre.hdf5',\n",
    "                                                             'Cifar10_Share_2_Inception_ex23_05_pre.hdf5',\n",
    "                                                             'Cifar10_Share_3_Inception_ex23_05_pre.hdf5',\n",
    "                                                             'Cifar10_Share_4_Inception_ex23_05_pre.hdf5'],\n",
    "                         'training_flag':[0, 0, 0, 0]\n",
    "    }\n",
    "df = pandas.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join('cifar10', 'cifar10v3', 'data_ex23_05.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join('cifar10', 'cifar10v3', 'data_ex23_05.csv')\n",
    "\n",
    "def synch_read_data(data_file=''):\n",
    "    while(True):\n",
    "        try:\n",
    "            df = pandas.read_csv(data_file, index_col=0)  \n",
    "            break                     \n",
    "        except:\n",
    "            #waiting for 10s\n",
    "            print(\"\\n\")\n",
    "            for i in range(10,0,-1):\n",
    "                print(\"re-read the file ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1) \n",
    "    return df  \n",
    "\n",
    "def synch_write_data(df,data_file=''):\n",
    "    while(True):\n",
    "        try:\n",
    "            df.to_csv(data_file)  \n",
    "            break                     \n",
    "        except:\n",
    "            #waiting for 10s\n",
    "            print(\"\\n\")\n",
    "            for i in range(10,0,-1):\n",
    "                print(\"re-read the file ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1) \n",
    "    return df  \n",
    "\n",
    "def get_pbest_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    pbest_value = row[1]\n",
    "    file_name = row[2]\n",
    "    return pbest_value, file_name\n",
    "\n",
    "def set_pbest_loc(row, pbest_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'pbest_value'] = pbest_value\n",
    "    synch_write_data(df,data_file)\n",
    "    \n",
    "def get_c_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    c_value = row[3]\n",
    "    file_name = row[4]\n",
    "    return c_value, file_name\n",
    "\n",
    "def set_c_loc(row, c_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'c_value'] = c_value\n",
    "    synch_write_data(df,data_file)   \n",
    "\n",
    "#    \n",
    "def get_pre_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    pre_value = row[5]\n",
    "    file_name = row[6]\n",
    "    return pre_value, file_name\n",
    "\n",
    "def set_pre_loc(row, pre_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'pre_value'] = pre_value\n",
    "    synch_write_data(df,data_file)\n",
    "    \n",
    "#training flag\n",
    "def get_training_flag(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    training_flag = row[7]\n",
    "    return training_flag\n",
    "\n",
    "def set_training_flag(row, training_status):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'training_flag'] = training_status\n",
    "    synch_write_data(df,data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance(w1, w2):\n",
    "    sqr_distance = 0\n",
    "    \n",
    "    w_np_1 = np.array(w1)\n",
    "    w_fl_1 = w_np_1.flatten()\n",
    "    w_np_2 = np.array(w2)\n",
    "    w_fl_2 = w_np_2.flatten()\n",
    "    \n",
    "    for i in range(len(w_np_1)):\n",
    "        x1_fl = w_fl_1[i].flatten()\n",
    "        x2_fl = w_fl_2[i].flatten()\n",
    "\n",
    "        tmp_dis = 0 \n",
    "        for j in range(len(x1_fl)):\n",
    "            tmp_dis = tmp_dis + (x1_fl[j]-x2_fl[j])**2\n",
    "\n",
    "    #     print(tmp_dis)\n",
    "        sqr_distance = sqr_distance + tmp_dis\n",
    "\n",
    "    return sqr_distance**(1/2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "\n",
    "#index of this pso\n",
    "pso_index = 0\n",
    "\n",
    "#number of neighbors (max=4)\n",
    "num_neighbors = 4\n",
    "#K coefficient\n",
    "M = 1\n",
    "u = 1\n",
    "\n",
    "tmp_acc = 0\n",
    "tmp_w = []\n",
    "pbest_acc = 0\n",
    "pbest_w = []\n",
    "\n",
    "#accelerator coefficient\n",
    "c1 = 0.5\n",
    "c2 = 0.5\n",
    "# w = 0.5\n",
    "\n",
    "r1 = 0\n",
    "r2 = 0\n",
    "\n",
    "results_stack_accuracy = []\n",
    "results_stack_val_accuracy = []\n",
    "results_stack_loss = []\n",
    "results_stack_val_loss = []\n",
    "\n",
    "#threshold\n",
    "# threshold = 0.97\n",
    "\n",
    "# #iteration control\n",
    "# i = 0\n",
    "# iter_max = 40\n",
    "\n",
    "warm_up = 0\n",
    "\n",
    "for index in range(0,warm_up):\n",
    "# while gbest_acc < threshold:\n",
    "    #save previous weight\n",
    "    model_mul.save_weights(savedfilename_pre)\n",
    "    result = model_mul.fit_generator(\n",
    "        generator = train_set, \n",
    "        steps_per_epoch = step_size_train,\n",
    "        validation_data = valid_set,\n",
    "        validation_steps = step_size_valid,\n",
    "        shuffle=True,\n",
    "        epochs=1,\n",
    "        callbacks=[checkpointer,tf.keras.callbacks.LearningRateScheduler(rand_scheduler)],\n",
    "    #     callbacks=[csv_logger, checkpointer, earlystopping],\n",
    "    #     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "        verbose=1) \n",
    "    \n",
    "    #save weights every iteration\n",
    "#     model_mul.save_weights(savedfilename)   \n",
    "    \n",
    "    tmp_acc = result.history.get('val_accuracy')[-1]\n",
    "    tmp_w = model_mul.get_weights()\n",
    "    tmp_lr = result.history.get('lr')[-1]\n",
    "    \n",
    "    #save current location\n",
    "    set_c_loc(pso_index,tmp_acc) \n",
    "    \n",
    "    if tmp_acc > pbest_acc:\n",
    "        pbest_acc = tmp_acc\n",
    "        pbest_w = tmp_w\n",
    "        #save person best location\n",
    "        set_pbest_loc(pso_index,pbest_acc)  \n",
    "        # save best model\n",
    "        model_mul.save_weights(savedfilename_best) \n",
    "    \n",
    "    results_stack_val_accuracy.append(result.history.get('val_accuracy')[-1])\n",
    "    results_stack_accuracy.append(result.history.get('accuracy')[-1])\n",
    "    results_stack_val_loss.append(result.history.get('val_loss')[-1])      \n",
    "    results_stack_loss.append(result.history.get('loss')[-1])\n",
    "    \n",
    "#     i = i + 1\n",
    "#     #exit after iteration gets max\n",
    "#     if i > iter_max:\n",
    "#         break\n",
    "\n",
    "#    \n",
    "# time synchronize\n",
    "number_of_pso = 4\n",
    "training_start_flag = 1\n",
    "training_finish_flag = 0\n",
    "\n",
    "#set initial training flag to start\n",
    "set_training_flag(pso_index, training_start_flag)\n",
    "\n",
    "for index in range(warm_up, epochs): \n",
    "# while i < iter_max:\n",
    "    #start training \n",
    "    set_training_flag(pso_index, training_start_flag)\n",
    "    print(get_training_flag(pso_index))\n",
    "    \n",
    "    #save previous weight\n",
    "    model_mul.save_weights(savedfilename_pre) \n",
    "    \n",
    "    result = model_mul.fit_generator(\n",
    "        generator = train_set, \n",
    "        steps_per_epoch = step_size_train,\n",
    "        validation_data = valid_set,\n",
    "        validation_steps = step_size_valid,\n",
    "        shuffle=True,\n",
    "        epochs=1,\n",
    "        callbacks=[checkpointer,tf.keras.callbacks.LearningRateScheduler(rand_scheduler)],\n",
    "    #     callbacks=[csv_logger, checkpointer, earlystopping],\n",
    "    #     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "        verbose=1) \n",
    "    \n",
    "    #save weights every iteration\n",
    "#     model_mul.save_weights(savedfilename)\n",
    "    \n",
    "    tmp_acc = result.history.get('val_accuracy')[-1]\n",
    "    tmp_w = model_mul.get_weights()\n",
    "    tmp_lr = result.history.get('lr')[-1]\n",
    "    \n",
    "    #save current location in scoreboard\n",
    "    set_c_loc(pso_index,tmp_acc) \n",
    "    \n",
    "    if tmp_acc > pbest_acc:\n",
    "        pbest_acc = tmp_acc\n",
    "        pbest_w = tmp_w\n",
    "        #save person best location\n",
    "        set_pbest_loc(pso_index,pbest_acc)  \n",
    "        # save best model\n",
    "        model_mul.save_weights(savedfilename_best)        \n",
    "\n",
    "    #set training flag to finish\n",
    "    set_training_flag(pso_index, training_finish_flag)  \n",
    "    print(get_training_flag(pso_index))\n",
    "        \n",
    "    # check if all PSOs is ready (flag==1)\n",
    "    while(True):\n",
    "        tmp_flag = 0\n",
    "        for flg_i in range(number_of_pso):\n",
    "            print(\"flg_i\", flg_i, \"flag\", get_training_flag(flg_i))\n",
    "            if(get_training_flag(flg_i) == 1):\n",
    "                tmp_flag = 1\n",
    "        if(tmp_flag==1):\n",
    "            #waiting for 60s\n",
    "            print(\"\\n\")\n",
    "            for i in range(60,0,-1):\n",
    "                print(\"waiting for ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1)    \n",
    "        else:\n",
    "            print(\"end of waiting\")\n",
    "            break                          \n",
    "        \n",
    "    r1 = random.uniform(0,1)\n",
    "    r2 = random.uniform(0,1)\n",
    "#     r3 = random.uniform(0,1)    \n",
    "    \n",
    "    #-----------nearest neighbor best--------------\n",
    "    #get neighbor weights\n",
    "    #1\n",
    "    neighbor_c_acc_1, name_file_1 = get_c_loc(0)\n",
    "    neighbor_c_acc_2, name_file_2 = get_c_loc(1)\n",
    "    neighbor_c_acc_3, name_file_3 = get_c_loc(2)\n",
    "    neighbor_c_acc_4, name_file_4 = get_c_loc(3)\n",
    "\n",
    "    #get pre loc\n",
    "    neighbor_pre_acc_1, name_pre_file_1 = get_pre_loc(0)\n",
    "    neighbor_pre_acc_2, name_pre_file_2 = get_pre_loc(1)\n",
    "    neighbor_pre_acc_3, name_pre_file_3 = get_pre_loc(2)\n",
    "    neighbor_pre_acc_4, name_pre_file_4 = get_pre_loc(3)  \n",
    "    \n",
    "    #clone model for weights change\n",
    "    model_clone = keras.models.clone_model(model_mul)\n",
    "    \n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_file_1))\n",
    "    neighbor_w_1 = model_clone.get_weights() \n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_file_2))\n",
    "    neighbor_w_2 = model_clone.get_weights()\n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_file_3))\n",
    "    neighbor_w_3 = model_clone.get_weights()\n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_file_4))\n",
    "    neighbor_w_4 = model_clone.get_weights()\n",
    "    \n",
    "    #clone model pre weights\n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_pre_file_1))\n",
    "    neighbor_pre_w_1 = model_clone.get_weights()     \n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_pre_file_2))\n",
    "    neighbor_pre_w_2 = model_clone.get_weights() \n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_pre_file_3))\n",
    "    neighbor_pre_w_3 = model_clone.get_weights()    \n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_pre_file_4))\n",
    "    neighbor_pre_w_4 = model_clone.get_weights()     \n",
    "    \n",
    "    distance_1 = find_distance(neighbor_w_1,neighbor_w_2)\n",
    "    distance_2 = find_distance(neighbor_w_1,neighbor_w_3)\n",
    "    distance_3 = find_distance(neighbor_w_1,neighbor_w_4)\n",
    "    \n",
    "    #find the closest neighbor\n",
    "    distances = list()\n",
    "    distances.append((0,0))\n",
    "    distances.append((1,distance_1))\n",
    "    distances.append((2,distance_2))\n",
    "    distances.append((3,distance_3))\n",
    "\n",
    "    print('distances unsorted', distances)\n",
    "    \n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    print('distances ', distances)\n",
    "    \n",
    "    neighbors_idx = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors_idx.append(distances[i][0])        \n",
    "    \n",
    "    print('neighbors ids ', neighbors_idx)\n",
    "    \n",
    "    #get neighbor bests from the list\n",
    "    neighbor_bests = list()\n",
    "    #remove first element (self distance)\n",
    "#     neighbors_idx.pop(0)\n",
    "    \n",
    "    for i in range(len(neighbors_idx)):\n",
    "        neighbor_best_tmp, name_file_neighbor_best_tmp = get_pbest_loc(neighbors_idx[i])\n",
    "        neighbor_bests.append((neighbors_idx[i],neighbor_best_tmp))\n",
    "        print('neighbor_idx ', neighbors_idx[i])\n",
    "        print('neighbor_best_tmp ', neighbor_best_tmp)\n",
    "    \n",
    "    # keep unsorted list of neighbor\n",
    "    neighbor_tmp = deepcopy(neighbor_bests)\n",
    "    \n",
    "    # sort the list for maximum accuracy   \n",
    "    neighbor_bests.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    print('neighbor best ', neighbor_bests)\n",
    "    #\n",
    "    neighbor_best_value, name_file_neighbor_best = get_pbest_loc(neighbor_bests[0][0])\n",
    "    print('name_file_neighbor_best ', name_file_neighbor_best)\n",
    "    \n",
    "    model_clone.load_weights(os.path.join('cifar10', 'cifar10v3', 'checkpoints', name_file_neighbor_best))\n",
    "    neighbor_best_w = model_clone.get_weights()  \n",
    "    #---------- end nearest neighbor best ----------\n",
    "    \n",
    "    #---------- cucker -----------------------------\n",
    "    particle_w_i = neighbor_w_1\n",
    "    sum_particle_tmp = 0\n",
    "    \n",
    "    #remove the fist (self)\n",
    "    neighbor_tmp.pop(0)\n",
    "    \n",
    "    for j in range(len(neighbor_tmp)):\n",
    "        if neighbor_tmp[j][0]==1:\n",
    "            particle_w_j = neighbor_w_2\n",
    "            particle_w_pre_j = neighbor_pre_w_2\n",
    "            distance_ij = distance_1\n",
    "            u = 0.2\n",
    "        elif neighbor_tmp[j][0]==2:    \n",
    "            particle_w_j = neighbor_w_3\n",
    "            particle_w_pre_j = neighbor_pre_w_3\n",
    "            distance_ij = distance_2\n",
    "            u = 0.2\n",
    "        elif neighbor_tmp[j][0]==3:    \n",
    "            particle_w_j = neighbor_w_4\n",
    "            particle_w_pre_j = neighbor_pre_w_4\n",
    "            distance_ij = distance_3\n",
    "            u = 0.2\n",
    "            \n",
    "        print('u ', u)\n",
    "        print('distance_ij ', distance_ij)\n",
    "        #sum(K/(1+distance)*(particle_w_j-particle_w_i)\n",
    "        sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \n",
    "        \n",
    "    #---------- end cucker -------------------------\n",
    "    \n",
    "    #---------- pbest ------------------------------\n",
    "    \n",
    "    #---------- end pbest --------------------------\n",
    "\n",
    "    #update networks' weights\n",
    "    #     w = c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(gbest_w)-np.array(tmp_w))\n",
    "    #     w = r1*np.array(pbest_w)+r2*np.array(tmp_w)+r3*np.array(gbest_w)\n",
    "    #     w = np.array(tmp_w)+tmp_lr*(c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(gbest_w)-np.array(tmp_w)))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp+c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "    final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = sum_particle_tmp+np.array(neighbor_best_w)\n",
    "#     final_weight = np.array(tmp_w)+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp\n",
    "\n",
    "    model_mul.set_weights(final_weight)\n",
    "    \n",
    "    print('After ---> epoch=', index, ' r1=',r1, ' r2=',r2, ' current acc=', tmp_acc, ' local best=', pbest_acc, \n",
    "          ' neighbor index=', neighbor_bests[0][0], ' neighbor best=', neighbor_best_value)  \n",
    "    \n",
    "    results_stack_val_accuracy.append(result.history.get('val_accuracy')[-1])\n",
    "    results_stack_accuracy.append(result.history.get('accuracy')[-1])\n",
    "    results_stack_val_loss.append(result.history.get('val_loss')[-1])      \n",
    "    results_stack_loss.append(result.history.get('loss')[-1])\n",
    "    \n",
    "#     i = i + 1\n",
    "        \n",
    "print(results_stack_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_stack_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_stack_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_stack_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_stack_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.history.get('val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.history.get('val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.history.get('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all PSOs is ready (flag==1)\n",
    "while(True):\n",
    "    try:\n",
    "        tmp_flag = 0\n",
    "        for flg_i in range(4):\n",
    "            print(\"flg_i\", flg_i, \"flag\", get_training_flag(flg_i))\n",
    "            for i in range(1,0,-1):\n",
    "                print(\"waiting for ....%2d\" %i, end=\"\\r\", flush=True)                \n",
    "            if(get_training_flag(flg_i) == 1):\n",
    "                tmp_flag = 1\n",
    "        if(tmp_flag==1):\n",
    "            #waiting for 60s\n",
    "            print(\"\\n\")\n",
    "            for i in range(1,0,-1):\n",
    "                print(\"waiting for ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1)        \n",
    "        else:\n",
    "            print(\"end of waiting\")\n",
    "            break \n",
    "    except:\n",
    "        #waiting for 10s\n",
    "        print(\"\\n\")\n",
    "        for i in range(10,0,-1):\n",
    "            print(\"exception ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"hello\")\n",
    "except:\n",
    "    print(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mul.load_weights(os.path.join('checkpoints', 'Cifar10_EfficientNetB7_299_STD.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "9it_mwRMLe-e",
    "outputId": "793a2e4b-544c-485f-b030-a2460c523a9b"
   },
   "outputs": [],
   "source": [
    "#Non-Groups\n",
    "#Split training and validation\n",
    "#Using Expert Data\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "# from keras.utils import multi_gpu_model\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import multiprocessing\n",
    "\n",
    "savedfilename = os.path.join('checkpoints', 'Cifar10_EfficientNetB7_299_STD_L2.hdf5')\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_acc', verbose=1, \n",
    "                          save_best_only=True, mode='max',save_weights_only=True)########\n",
    "\n",
    "epochs = 15##!!!\n",
    "lr = 1e-5\n",
    "decay = lr/epochs\n",
    "optimizer = Adam(lr=lr, decay=decay)\n",
    "\n",
    "model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "result = model_mul.fit_generator(\n",
    "    generator = train_set_1, \n",
    "    steps_per_epoch = step_size_train,\n",
    "    validation_data = valid_set,\n",
    "    validation_steps = step_size_valid,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlystopping, checkpointer],\n",
    "#     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "    verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mul.load_weights(os.path.join('checkpoints', 'Cifar10_EfficientNetB7_299_STD_L2.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Groups\n",
    "#Split training and validation\n",
    "#Using Expert Data\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "# from keras.utils import multi_gpu_model\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import multiprocessing\n",
    "\n",
    "savedfilename = os.path.join('checkpoints', 'Cifar10_EfficientNetB7_299_STD_L3.hdf5')\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_acc', verbose=1, \n",
    "                          save_best_only=True, mode='max',save_weights_only=True)########\n",
    "\n",
    "epochs = 15##!!!\n",
    "lr = 1e-6\n",
    "decay = lr/epochs\n",
    "optimizer = Adam(lr=lr, decay=decay)\n",
    "\n",
    "model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "result = model_mul.fit_generator(\n",
    "    generator = train_set_1, \n",
    "    steps_per_epoch = step_size_train,\n",
    "    validation_data = valid_set,\n",
    "    validation_steps = step_size_valid,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlystopping, checkpointer],\n",
    "#     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "    verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZBm5kY_TxMv"
   },
   "outputs": [],
   "source": [
    "# #Using multiple models if more than 1 GPU\n",
    "# NUM_GPU = 4\n",
    "# if NUM_GPU != 1:\n",
    "#     model_mul = multi_gpu_model(model_1, gpus=NUM_GPU)\n",
    "\n",
    "model_mul.load_weights(os.path.join('checkpoints', 'Cifar10_EfficientNetB7_299_STD_L3.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time, os\n",
    "from math import ceil\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    predict1=model_mul.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "# else:\n",
    "#     predict1=model.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "    \n",
    "predicted_class_indices=np.argmax(predict1,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions1 = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"file_name\":filenames,\n",
    "                      \"predicted1\":predictions1,\n",
    "                      })\n",
    "results.to_csv('Cifar10_EfficientB7_299_STD_2708.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_Eff_B5_345_1511_v1.csv /home/bribeiro/Phong/Nat19/Cifar10_Eff_B5_345_1511_v1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('pred_npy','Cifar10_EfficientB7_299_STD_L3.npy'), predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 72\n",
    "\n",
    "#Crop-Official Test\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Generate random crops from the image batches\"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "\n",
    "test_datagen_crop = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "testing_set_crop = test_datagen_crop.flow_from_directory('test_resized_345',\n",
    "                                                 target_size = (370, 370),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "#customized generator\n",
    "test_crops = crop_generator(testing_set_crop, 345)\n",
    "\n",
    "step_size_test_crop = ceil(testing_set_crop.n/testing_set_crop.batch_size)\n",
    "\n",
    "tta_steps = 4\n",
    "# predictions = []\n",
    "\n",
    "# import tensorflow as tf\n",
    "# with tf.device('/gpu:0'):\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    testing_set_crop.reset()\n",
    "    if NUM_GPU != 1:\n",
    "        preds=model_mul.predict_generator(test_crops, \n",
    "                                           steps = step_size_test_crop,\n",
    "#                                            max_queue_size=16,\n",
    "#                                                use_multiprocessing=True,\n",
    "#                                            workers=1,\n",
    "                                           verbose=1)    \n",
    "#     else:\n",
    "#         preds=model.predict_generator(test_crops, \n",
    "#                                            steps = step_size_test_crop,\n",
    "#                                            max_queue_size=16,\n",
    "# #                                                use_multiprocessing=True,\n",
    "#                                            workers=1,\n",
    "#                                            verbose=1)  \n",
    "#     preds=model_2.predict_generator(test_crops,steps = step_size_test_crop,verbose=1)  \n",
    "    predictions.append(preds)\n",
    "\n",
    "mean_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=testing_set_crop.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Cifar10_Eff_B5_345_STD_tta_7.csv')\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp Cifar10_Eff_B5_345_STD_tta_7.csv /home/bribeiro/Phong/Nat19/Cifar10_Eff_B5_345_STD_tta_7.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('pred_npy','Cifar10_Eff_B5_345_L2_TTA3.npy'), mean_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Groups\n",
    "#Split training and validation\n",
    "#Using Expert Data\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "# from keras.utils import multi_gpu_model\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import multiprocessing\n",
    "\n",
    "savedfilename = os.path.join('checkpoints', 'Cifar100_Eff_B7_299_STD_L3.hdf5')\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_acc', verbose=1, \n",
    "                          save_best_only=True, mode='max',save_weights_only=True)########\n",
    "\n",
    "epochs = 15##!!!\n",
    "lr = 1e-6\n",
    "decay = lr/epochs\n",
    "optimizer = Adam(lr=lr, decay=decay)\n",
    "\n",
    "model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "result = model_mul.fit_generator(\n",
    "    generator = train_set, \n",
    "    steps_per_epoch = step_size_train,\n",
    "    validation_data = valid_set,\n",
    "    validation_steps = step_size_valid,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlystopping, checkpointer],\n",
    "#     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "    verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using multiple models if more than 1 GPU\n",
    "# NUM_GPU = 4\n",
    "# if NUM_GPU != 1:\n",
    "#     model_mul = multi_gpu_model(model_1, gpus=NUM_GPU)\n",
    "\n",
    "model_mul.load_weights(os.path.join('checkpoints', 'Cifar100_Eff_B7_299_STD_L3.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time, os\n",
    "from math import ceil\n",
    "\n",
    "# PREDICT ON OFFICIAL TEST\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "batch_size = 36\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('train_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_resized_299',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    predict1=model_mul.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "# else:\n",
    "#     predict1=model.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "    \n",
    "predicted_class_indices=np.argmax(predict1,axis=1)\n",
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions1 = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"file_name\":filenames,\n",
    "                      \"predicted1\":predictions1,\n",
    "                      })\n",
    "results.to_csv('Cifar100_Eff_B0_299_1108_v1.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('pred_npy','Cifar100_Eff_B7_299_STD_L3.npy'), predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cifar10_efficientnet_B5_345_T2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
